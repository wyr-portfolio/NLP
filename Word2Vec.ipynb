{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2120b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Trista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4a493",
   "metadata": {},
   "source": [
    "# Convert Words to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665afd9",
   "metadata": {},
   "source": [
    "Load text, remove special characters and stopwords if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a14f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(stopwords='no'):\n",
    "    file_contents = []\n",
    "    with open('word2vec.txt') as f:\n",
    "        file_contents = f.read()\n",
    "\n",
    "    sentences = tokenize.sent_tokenize(file_contents)\n",
    "    text = []\n",
    "\n",
    "    for s in sentences:\n",
    "        s = re.sub(r\"[^a-zA-Z0-9]+\", ' ', s)\n",
    "        s = s.lower()\n",
    "\n",
    "        if stopwords == 'yes' and len(s) > 1: \n",
    "            sentence = remove_stopwords(s)\n",
    "        \n",
    "        text.append(sentence)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split(' ')\n",
    "    sentence = ''\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            sentence += word + ' '\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f307a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_text(stopwords='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9e155",
   "metadata": {},
   "source": [
    "Build dictionaries to store word indexes (word to index & index to word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fb2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(text):\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    corpus = []\n",
    "    index = 0\n",
    "\n",
    "    for s in text:\n",
    "        for word in s.split(' '):\n",
    "            word = word.lower()\n",
    "\n",
    "            if len(word) > 0:\n",
    "                corpus.append(word)\n",
    "                if word_to_index.get(word) == None:\n",
    "                    word_to_index.update({word: index})\n",
    "                    index_to_word.update({index: word})\n",
    "                    index += 1\n",
    "    \n",
    "    return word_to_index, index_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c36b2",
   "metadata": {},
   "source": [
    "Generate one-hot-vector to each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbf9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vec(target_word, context_words, corpus, word_to_index, index_to_word):\n",
    "    size_vocab = len(word_to_index)\n",
    "    target_word_vec = np.zeros(size_vocab)\n",
    "    context_words_vec = np.zeros(size_vocab)  # set the context words into one vector to save space\n",
    "\n",
    "    target_index = word_to_index.get(target_word)\n",
    "    target_word_vec[target_index] = 1\n",
    "\n",
    "    for word in context_words:\n",
    "        context_index = word_to_index.get(word)\n",
    "        context_words_vec[context_index] = 1\n",
    "        \n",
    "    return target_word_vec, context_words_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7699fd",
   "metadata": {},
   "source": [
    "Prepare the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77efe740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(window_size, corpus, word_to_index, index_to_word):\n",
    "    training_data = []\n",
    "\n",
    "    for i, word in enumerate(corpus):\n",
    "        context_word = []\n",
    "        if i == 0:\n",
    "            context_word = corpus[(i + 1): (i + 1 + window_size)]      \n",
    "        elif i == len(corpus) - 1:\n",
    "            context_word = corpus[(i - window_size): i]\n",
    "        else:\n",
    "            if i - window_size < 0:\n",
    "                context_word = corpus[0: i] + corpus[(i + 1): (i + 1 + window_size)]\n",
    "            elif i + window_size > len(corpus):\n",
    "                context_word = corpus[(i - window_size): i] + corpus[(i + 1): len(corpus)]              \n",
    "            else:\n",
    "                context_word = corpus[(i - window_size): i] + corpus[(i + 1): (i + 1 + window_size)]\n",
    "            \n",
    "        target_word_vec, context_words_vec = one_hot_vec(word, context_word, corpus, word_to_index, index_to_word)\n",
    "        training_data.append([target_word_vec, context_words_vec])\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f8e31",
   "metadata": {},
   "source": [
    "Test for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d09389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['natural language processing and machine learning is fun and exciting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00f9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "word_to_index, index_to_word, corpus = build_dict(text)\n",
    "training_words = generate_training_data(window_size, corpus, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe9219",
   "metadata": {},
   "source": [
    "# Model Training - Skip-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a375714",
   "metadata": {},
   "source": [
    "Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb08e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(input_weights, hidden_weights, X):\n",
    "    hidden_layer = np.dot(X, input_weights)\n",
    "    output_layer = np.dot(hidden_layer, hidden_weights)\n",
    "    y_hat = softmax(output_layer)\n",
    "    \n",
    "    return y_hat, hidden_layer, output_layer\n",
    "\n",
    "def softmax(Z):\n",
    "    e_x = np.exp(Z - np.max(Z))\n",
    "    return e_x / e_x.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6017b8b",
   "metadata": {},
   "source": [
    "Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cadcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_fun(context_words_vec, y_hat):\n",
    "    vector_list = []\n",
    "    length = len(context_words_vec)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(length):\n",
    "        if context_words_vec[i] == 1:\n",
    "            vec = [0 if k != i else 1 for k in range(length)]\n",
    "            vector_list.append(vec)\n",
    "\n",
    "    error = np.zeros(length)\n",
    "    for vec in vector_list:\n",
    "        error += np.subtract(y_hat, vec)\n",
    "    errors.append(error)\n",
    "    \n",
    "    errors = np.array(errors)\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3206d7",
   "metadata": {},
   "source": [
    "Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7174ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y_hat, input_weights, hidden_weights, \n",
    "              error, hidden_layer, learning_rate, target_word_vec):\n",
    "    delta_input_weights = np.outer(target_word_vec, np.dot(hidden_weights, error.T))\n",
    "    delta_hidden_weights = np.outer(hidden_layer, error)\n",
    "    \n",
    "    # update weights\n",
    "    input_weights_updated = input_weights - learning_rate * delta_input_weights\n",
    "    hidded_weights_updated = hidden_weights - learning_rate * delta_hidden_weights\n",
    "    \n",
    "    return input_weights_updated, hidded_weights_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776ebfe",
   "metadata": {},
   "source": [
    "Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cffe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(output_layer, context_words_vec): \n",
    "    total_loss = 0\n",
    "    \n",
    "    sum1 = 0\n",
    "\n",
    "    for index in np.where(context_words_vec == 1)[0]:\n",
    "        sum1 += output_layer[index]\n",
    "\n",
    "    sum1 = -sum1\n",
    "    sum2 = len(np.where(context_words_vec == 1)[0]) * np.log(np.sum(np.exp(output_layer)))\n",
    "\n",
    "    loss = sum1 + sum2\n",
    "    total_loss += loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc35e3",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b0bd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(word_embedding_size, training_words, learning_rate, num_epoch, vocab_size):\n",
    "\n",
    "    input_weights = np.random.uniform(-1, 1, (vocab_size, word_embedding_size))\n",
    "    hidden_weights = np.random.uniform(-1, 1, (word_embedding_size, vocab_size))\n",
    "    \n",
    "    epoch_loss = []\n",
    "    weights_1 = []\n",
    "    weights_2 = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        for target_word, context_word in training_words:\n",
    "            y_hat, hidden_layer, output_layer = forward_prop(input_weights, hidden_weights, target_word)\n",
    "            error = error_fun(context_word, y_hat)\n",
    "            input_weights, hidden_weights = back_prop(y_hat, input_weights, hidden_weights, \n",
    "                          error, hidden_layer, learning_rate, target_word)\n",
    "            loss = loss_fun(output_layer, context_word)\n",
    "            total_loss += loss\n",
    "    \n",
    "        epoch_loss.append(loss)\n",
    "        weights_1.append(input_weights)\n",
    "        weights_2.append(hidden_weights)\n",
    "        \n",
    "    \n",
    "    return epoch_loss, weights_1, weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065a474",
   "metadata": {},
   "source": [
    "Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fa0e622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x165d13c9c70>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_num = 100\n",
    "learning_rate = 0.01\n",
    "word_embedding_size = 100\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "epoch_loss, weights_1, weights_2 = train(word_embedding_size, training_words, learning_rate, epoch_num, vocab_size)\n",
    "\n",
    "plt.plot(list(range(epoch_num)), epoch_loss, '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1392cf",
   "metadata": {},
   "source": [
    "Compute vector distance (Input vector, returns nearest words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c05d1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word, weight, word_to_index, index_to_word, vocab_size):\n",
    "    \n",
    "    # Get the index of the word from the dictionary\n",
    "    index = word_to_index[word]\n",
    "    \n",
    "    # Get the corresponding weights for the word\n",
    "    word_vector_1 = weight[index]\n",
    "    \n",
    "    \n",
    "    word_similarity = {}\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        \n",
    "        word_vector_2 = weight[i]\n",
    "        \n",
    "        theta_sum = np.dot(word_vector_1, word_vector_2)\n",
    "        theta_den = np.linalg.norm(word_vector_1) * np.linalg.norm(word_vector_2)\n",
    "        theta = theta_sum / theta_den\n",
    "        \n",
    "        word = index_to_word[i]\n",
    "        word_similarity[word] = theta\n",
    "    \n",
    "    return word_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da175c3",
   "metadata": {},
   "source": [
    "Print a dataframe for similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ace4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_similar_words(top_n_words, weight, msg, words_subset, vocab_size):\n",
    "    \n",
    "    columns=[]\n",
    "    \n",
    "    for i in range(0,len(words_subset)):\n",
    "        columns.append('similar:' +str(i+1) )\n",
    "        \n",
    "    df = pd.DataFrame(columns=columns,index=words_subset)\n",
    "    df.head()\n",
    "    \n",
    "    row = 0\n",
    "    for word in words_subset:\n",
    "        \n",
    "        #Get the similarity matrix for the word: word\n",
    "        similarity_matrix = cosine_similarity(word, weight, word_to_index, index_to_word, vocab_size)\n",
    "        col = 0\n",
    "        \n",
    "        #Sort the top_n_words\n",
    "        words_sorted = dict(sorted(similarity_matrix.items(), key=lambda x: x[1], reverse=True)[1:top_n_words+1])\n",
    "        \n",
    "        #Create a dataframe to display the similarity matrix\n",
    "        for similar_word,similarity_value in words_sorted.items():\n",
    "            df.iloc[row][col] = (similar_word,round(similarity_value,2))\n",
    "            col += 1\n",
    "        row += 1\n",
    "    styles = [dict(selector='caption', \n",
    "    props=[('text-align', 'center'),('font-size', '20px'),('color', 'red')])] \n",
    "    df = df.style.set_properties(**\n",
    "                       {'color': 'green','border-color': 'blue','font-size':'14px'}\n",
    "                      ).set_table_styles(styles).set_caption(msg)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ccecfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_457a9_ caption {\n",
       "          text-align: center;\n",
       "          font-size: 20px;\n",
       "          color: red;\n",
       "    }#T_457a9_row0_col0,#T_457a9_row0_col1,#T_457a9_row0_col2,#T_457a9_row0_col3,#T_457a9_row0_col4,#T_457a9_row1_col0,#T_457a9_row1_col1,#T_457a9_row1_col2,#T_457a9_row1_col3,#T_457a9_row1_col4,#T_457a9_row2_col0,#T_457a9_row2_col1,#T_457a9_row2_col2,#T_457a9_row2_col3,#T_457a9_row2_col4,#T_457a9_row3_col0,#T_457a9_row3_col1,#T_457a9_row3_col2,#T_457a9_row3_col3,#T_457a9_row3_col4,#T_457a9_row4_col0,#T_457a9_row4_col1,#T_457a9_row4_col2,#T_457a9_row4_col3,#T_457a9_row4_col4{\n",
       "            color:  green;\n",
       "            border-color:  blue;\n",
       "            font-size:  14px;\n",
       "        }</style><table id=\"T_457a9_\" ><caption>sim_matrix for : dimension_100_epochs_100_window_size_2</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >similar:1</th>        <th class=\"col_heading level0 col1\" >similar:2</th>        <th class=\"col_heading level0 col2\" >similar:3</th>        <th class=\"col_heading level0 col3\" >similar:4</th>        <th class=\"col_heading level0 col4\" >similar:5</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_457a9_level0_row0\" class=\"row_heading level0 row0\" >reports</th>\n",
       "                        <td id=\"T_457a9_row0_col0\" class=\"data row0 col0\" >('writing', 0.31)</td>\n",
       "                        <td id=\"T_457a9_row0_col1\" class=\"data row0 col1\" >('monthly', 0.27)</td>\n",
       "                        <td id=\"T_457a9_row0_col2\" class=\"data row0 col2\" >('however', 0.26)</td>\n",
       "                        <td id=\"T_457a9_row0_col3\" class=\"data row0 col3\" >('imprisonment', 0.26)</td>\n",
       "                        <td id=\"T_457a9_row0_col4\" class=\"data row0 col4\" >('want', 0.26)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_457a9_level0_row1\" class=\"row_heading level0 row1\" >promises</th>\n",
       "                        <td id=\"T_457a9_row1_col0\" class=\"data row1 col0\" >('term', 0.31)</td>\n",
       "                        <td id=\"T_457a9_row1_col1\" class=\"data row1 col1\" >('children', 0.3)</td>\n",
       "                        <td id=\"T_457a9_row1_col2\" class=\"data row1 col2\" >('undermines', 0.26)</td>\n",
       "                        <td id=\"T_457a9_row1_col3\" class=\"data row1 col3\" >('brooks', 0.25)</td>\n",
       "                        <td id=\"T_457a9_row1_col4\" class=\"data row1 col4\" >('wallace', 0.25)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_457a9_level0_row2\" class=\"row_heading level0 row2\" >ride</th>\n",
       "                        <td id=\"T_457a9_row2_col0\" class=\"data row2 col0\" >('divorce', 0.32)</td>\n",
       "                        <td id=\"T_457a9_row2_col1\" class=\"data row2 col1\" >('mid', 0.28)</td>\n",
       "                        <td id=\"T_457a9_row2_col2\" class=\"data row2 col2\" >('moment', 0.27)</td>\n",
       "                        <td id=\"T_457a9_row2_col3\" class=\"data row2 col3\" >('letter', 0.25)</td>\n",
       "                        <td id=\"T_457a9_row2_col4\" class=\"data row2 col4\" >('decorated', 0.23)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_457a9_level0_row3\" class=\"row_heading level0 row3\" >aunt</th>\n",
       "                        <td id=\"T_457a9_row3_col0\" class=\"data row3 col0\" >('rifle', 0.3)</td>\n",
       "                        <td id=\"T_457a9_row3_col1\" class=\"data row3 col1\" >('posted', 0.29)</td>\n",
       "                        <td id=\"T_457a9_row3_col2\" class=\"data row3 col2\" >('wife', 0.27)</td>\n",
       "                        <td id=\"T_457a9_row3_col3\" class=\"data row3 col3\" >('lines', 0.26)</td>\n",
       "                        <td id=\"T_457a9_row3_col4\" class=\"data row3 col4\" >('love', 0.25)</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_457a9_level0_row4\" class=\"row_heading level0 row4\" >conspirators</th>\n",
       "                        <td id=\"T_457a9_row4_col0\" class=\"data row4 col0\" >('five', 0.29)</td>\n",
       "                        <td id=\"T_457a9_row4_col1\" class=\"data row4 col1\" >('captured', 0.28)</td>\n",
       "                        <td id=\"T_457a9_row4_col2\" class=\"data row4 col2\" >('murder', 0.28)</td>\n",
       "                        <td id=\"T_457a9_row4_col3\" class=\"data row4 col3\" >('considers', 0.26)</td>\n",
       "                        <td id=\"T_457a9_row4_col4\" class=\"data row4 col4\" >('mate', 0.25)</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x165c4a54880>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words = 5\n",
    "words_subset = []\n",
    "words_subset = np.random.choice(list(word_to_index.keys()),top_n_words)\n",
    "\n",
    "print_similar_words(\n",
    "    top_n_words,\n",
    "    weights_1[-1],\n",
    "    'sim_matrix for : dimension_' + str(word_embedding_size) + '_epochs_' + str(epoch_num) + '_window_size_' +str(window_size),\n",
    "    words_subset, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaf37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
